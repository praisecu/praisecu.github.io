<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:fb="http://ogp.me/ns/fb#">

<head>
  <!-- Title and Logo -->
  <title>PRAISe at CU Boulder</title>
  <link rel="icon" type="image/x-icon" href="img/logo/praise.webp">
  <!-------------------->

  <googleanalytics>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-CW9BT60PE9"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-CW9BT60PE9');
    </script>
  </googleanalytics>

  <meta-info>
    <meta name="viewport" content="width=device-width, initial-scale=0.8, maximum-scale=0.45, user-scalable=no">
    <meta name="google-site-verification" content="jENy3ocy5ni0dYJjv42rrZ85uz_oE0teegM_2JoajDQ" />
    <!-- <meta property="og:image" content="http://prg.cs.umd.edu/img/logo/prg-black.png" /> -->
    <!-- <meta http-equiv="X-UA-Compatible" content="IE=edge"> -->
    <!-- <meta name="viewport" content="width=768px, initial-scale=1.0, maximum-scale=1.0"> -->
    <!-- <meta property="og:image" content="http://prg.cs.umd.edu/img/thumbnail.png" /> -->
    <meta name="description" content="">
    <meta name="author" content=""> 
  </meta-info>

  <myimports>
    <!-- Team.css -->
    <link href="css/team.css" rel="stylesheet"> 
    <!-- Web Fonts -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" />
    <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,600,300" rel="stylesheet" type="text/css">

    <!-- Nunito Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,opsz,wght@0,6..12,200..1000;1,6..12,200..1000&display=swap" rel="stylesheet">

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <!-- Flaticon CSS -->
    <link href="fonts/flaticon/flaticon.css" rel="stylesheet">
    <!-- font-awesome CSS -->
    <link rel="stylesheet" href="path/to/bootstrap/css/bootstrap.min.css">
    <link href="css/font-awesome.min.css" rel="stylesheet">
    <script src="https://kit.fontawesome.com/4ad8d74216.js" crossorigin="anonymous"></script>
    <!-- Offcanvas CSS -->
    <link href="css/hippo-off-canvas.css" rel="stylesheet">
    <!-- animate CSS -->
    <link href="css/animate.css" rel="stylesheet">
    <!-- language CSS -->
    <link href="css/language-select.css" rel="stylesheet">
    <!-- owl.carousel CSS -->
    <link href="owl.carousel/assets/owl.carousel.css" rel="stylesheet">
    <!-- magnific-popup -->
    <link href="css/magnific-popup.css" rel="stylesheet">
    <!-- Main menu -->
    <link href="css/menu.css" rel="stylesheet">
    <!-- Template Common Styles -->
    <link href="../css/template.css" rel="stylesheet">
    <!-- Custom CSS -->
    <link href="css/style.css" rel="stylesheet">
    <!-- Responsive CSS -->
    <link href="../css/responsive.css" rel="stylesheet">
    <!-- Research CSS -->
    <link href="css/research.css" rel="stylesheet">
    <!-- ai-font for Research Fonts like Google Scholar -->
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <!-- New PRAISe style CSS -->
    <link href="css/praise-style.css" rel="stylesheet">
    <!-- CSS for Cards -->
    <link href="css/cards.css" rel="stylesheet">
    <!-- Import MathJAX -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <script src="js/vendor/modernizr-2.8.1.min.js"></script>
    <script src="js/header.js" type="text/javascript" defer></script>
    <script src="js/footer.js" type="text/javascript" defer></script>

   
    <!-- HTML5 Shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!-- [if lt IE 9]> -->
    <!-- <script src="js/vendor/html5shim.js"></script> -->
    <!-- <script src="js/vendor/respond.min.js"></script> -->
    <!-- <![endif] -->
  </myimports>


  </head>

  <!-- Header is imported from header.js -->
  <header-component></header-component>


  <section id="fall2024" class="cta-section">
    <div class="container">
        <h1 class="first-section">
          ROBO/MCEN 5228: <b>Advanced Computer Vision
        </h1><div class="row text-center">
          <div class="pub-text"><h3>Geometry and Learning-based Methods in Computer Vision</h3></div><hr>
          <!-- <h2 class="section-title">Fall 2024</h2><hr><div class="pub-img"> -->
        <!-- </div><a><img src="../img/teaching/AdvancedCV-Class-Banner/Slide1.PNG"  style="border-radius: 30px;" /> </a></div> -->
        
        <!-- <div class="pub"> -->
        <!-- <div class="item"> -->
          <!-- <div class="featured-image"><div class="pub-img"> -->
          <!-- </div></div><br> -->
          <!-- <div class="pub-rest" style="padding-top:0px"><br><br> -->
            <div class="row text-center"><h2 class="section-title">Project 3: Part 1 | Structure from Motion</h2><div class="pub-img"></div>


      <!-- <div class="row text-center"><hr><h2 class="section-title">Table of Contents</h2><br><div class="pub-img"></div></div>   -->
      <div class="pub-text"><h4 style="text-align:justify;">

      <style table>
        table {
          width: 100%;
          border-collapse: collapse;
          text-align: center;

        }
        th{
          border-left: 1px solid rgba(26, 26, 26, 0.459);
        }
        td{
          border-right: 1px solid rgba(218, 218, 218, 0.445);
        }
        th, td {
          padding: 8px;
          text-align: center;
          /* border-left:solid black 1px; */
          /* border-top:solid black 1px; */
        }
        
        tr:nth-child(odd) {
          background-color: #97940021;
        }

        a:link {
          color:#7c6700;
          font-weight: 500;
          text-decoration: underline;
        }
        th {
          background-color: #2c2500;
          color: white; /* White font color */
        }


        /* Making the table curve */
        th:first-of-type {
          border-top-left-radius: 10px;
          border: 0px solid black;

        }
        th:last-of-type {
          border-top-right-radius: 10px;
        }
        tr:last-of-type td:first-of-type {
          border-bottom-left-radius: 10px;
        }
        tr:last-of-type td:last-of-type {
          border-bottom-right-radius: 10px;
        }
        td:last-of-type {
          border-right: 0px;
        }
        
</style>




<hr>
<h3 class="tutorial">Table of Contents</h3><hr>
<style>
  li {
      text-align: left;
      margin: 0;
      padding: 5px 20px;
      font-weight: 400;
      font-size: 18px;
  }
</style>
<ol>
    <li>Deadline</li>
    <li>Introduction</li>
    <li><b>Part 1: Structure From Motion</b>
        <ul>
            <li>3.1.  Feature Matching</li>
            <li>3.2.  Estimating Fundamental Matrix
                <ul>
                    <li>3.2.1  Epipolar Geometry</li>
                    <li>3.2.2  Fundamental Matrix</li>
                    <li>3.2.3  RANSAC Outlier Rejection</li>
                </ul>
            </li>
            <li>3.3 Estimate Essential Matrix from Fundamental Matrix</li>
            <li>3.4 Estimate Camera Pose from Essential Matrix</li>
            <li>3.5 Check for Cheirality Condition using Triangulation
                <ul>
                    <li>3.5.1 Non-Linear Triangulation</li>
                </ul>
            </li>
            <li>3.6 Perspective-n-points
                <ul>
                    <li>3.6.1 Linear Camera Pose Estimation</li>
                    <li>3.6.2 PnP RANSAC</li>
                    <li>3.6.3  NonLinear PnP</li>
                </ul>
            </li>
            <li>3.7 Bundle Adjustment
                <ul>
                    <li>3.7.1 Visibility Matrix</li>
                    <li>3.7.2 Bundle Adjustment Optimization</li>
                </ul>
            </li>
        </ul>
    </li>
    <li>Putting the pipeline together</li>
    <li>Notes about Data Set</li>
    <li>Submission Guidelines
        <ul>
            <li>6.1. File tree and naming</li>
            <li>6.2. Report</li>
        </ul>
    </li>
    <li>Collaboration Policy</li>
</ol>




<hr>
  <style>
    p.tutorial{
      text-align: left;
      font-size: 18px;
      font-style: normal;
      text-align: justify;
      line-height: 130%;
      font-weight: 400;
    }
    h2.tutorial{
      text-align: left;
      font-size: 28px;
      font-weight: 400;
      font-style: normal;
      color: #2c2500
    }
    h3.tutorial, h4.tutorial{
      text-align: left;
    }
  </style>

  <h2 class="tutorial" id="due">1. Deadline</h3>
  <p class="tutorial">11:59PM, Nov 27, 2024. To be submitted with Part 2 (Gaussian Splatting) in a group of 2.</p><hr>

  
  <h2 class="tutorial" id="intro">2. Introduction</h2>
  <p class="tutorial">We have been playing with images for so long, mostly in 2D scenes. Recall Panorama project</a> where we stitched multiple images with about 30-50% common features between a couple of images. Now let's learn how to <strong>reconstruct a 3D scene and simultaneously obtain the camera poses</strong> of a monocular camera w.r.t. the given scene. This procedure is known as Structure from Motion (SfM). As the name suggests, you are creating the entire <strong>rigid</strong> structure from a set of images with different viewpoints (or equivalently a camera in motion). A few years ago, Agarwal et al. published <a href="http://grail.cs.washington.edu/rome/rome_paper.pdf">Building Rome in a Day</a>, in which they reconstructed the entire city just by using a large collection of photos from the Internet. Ever heard of Microsoft <a href="https://en.wikipedia.org/wiki/Photosynth">Photosynth?</a> <em>Fascinating, isn't it!?</em> There are a few open-source SfM algorithms available online like <a href="http://ccwu.me/vsfm/">VisualSFM</a>. Another open source SfM pipeline with GUI is <a href="https://colmap.github.io/">COLMAP!</a><em> Try them out!</em></p>
  
  <p>Let's learn how to recreate such an algorithm. There are a few steps that collectively form SfM:</p>
  <ul style="list-style-type:square">
      <li><strong>Feature Matching</strong> and Outlier rejection using <strong>RANSAC</strong></li>
      <li>Estimating <strong>Fundamental Matrix</strong></li>
      <li>Estimating <strong>Essential Matrix</strong> from Fundamental Matrix</li>
      <li>Estimate <strong>Camera Pose</strong> from Essential Matrix</li>
      <li>Check for <strong>Cheirality Condition</strong> using <strong>Triangulation</strong></li>
      <li><strong>Perspective-n-Point</strong></li>
      <li><strong>Bundle Adjustment</strong></li>
  </ul>
  
  <hr>
  <h2 class="tutorial" id="trad">3. Traditional Approach to the SfM Problem</h2><hr>
  <h4 class="tutorial" id="featmatch">3.1. Feature Matching, Fundamental Matrix, and RANSAC</h3><hr>
  <p class="tutorial">We have already learned about keypoint matching using SIFT keypoints and descriptors (Recall Project 1: Panorama Stitching). It is important to refine the matches by rejecting outlier correspondences. Before rejecting the correspondences, let us first understand what the Fundamental matrix is!</p><hr>


<img src="https://cmsc733.github.io/assets/2019/p3/featmatch.png" style="border-radius: 30px;" width="60%">
    <h5>Figure 1: Feature matching between two images from different views.</h5>
<hr>


  <h3 class="tutorial">3.2. Estimating Fundamental Matrix</h2><hr>
  <p class="tutorial">The fundamental matrix, denoted by \( \mathbf{F} \), is a \( 3 \times 3 \) (rank 2) matrix that relates the corresponding set of points in two images from different views (or stereo images). But in order to understand what the fundamental matrix actually is, we need to understand what <strong>epipolar geometry</strong> is! The epipolar geometry is the intrinsic projective geometry between two views. It only depends on the cameras' internal parameters (denoted by the \( \mathbf{K} \) matrix) and the relative pose, i.e., it is independent of the scene structure.</p><hr>


  <h3 class="tutorial">3.2.1. Epipolar Geometry</h3><hr>
 
  <p class="tutorial">Let's say a point \( \mathbf{X} \) in the 3D-space (viewed in two images) is captured as \( \mathbf{x} \) in the first image and \( \mathbf{x'} \) in the second. 
  <em>Can you think how to formulate the relation between the corresponding image points \( \mathbf{x} \) and \( \mathbf{x'} \)?</em> 
  Consider Fig. 2. Let \( \mathbf{C} \) and \( \mathbf{C'} \) be the respective camera centers, which form the baseline for the stereo system. 
  Clearly, the points \( \mathbf{x} \), \( \mathbf{x'} \), and \( \mathbf{X} \) (or \( \mathbf{C} \), \( \mathbf{C'} \), and \( \mathbf{X} \)) are coplanar, 
  <em>i.e.</em>:

  \[
  \mathbf{\overrightarrow{Cx}} \cdot \left( \mathbf{\overrightarrow{CC'}} \times \mathbf{\overrightarrow{C'x'}} \right) = 0
  \]
  
  The plane formed can be denoted by \( \pi \). Since these points are coplanar, the rays back-projected from \( \mathbf{x} \) and \( \mathbf{x'} \) intersect at \( \mathbf{X} \). 
  This is the most significant property in searching for a correspondence.</p><hr>

  <img src="https://cmsc733.github.io/assets/2019/p3/epipole1.png" style="border-radius: 30px;" width="60%">
  <h5>Figure 2(a): Epipolar planes and epipolar lines</h5>
  <hr>  
  <img src="https://cmsc733.github.io/assets/2019/p3/epipole2.png" style="border-radius: 30px;" width="60%">
  <h5>Figure 2(b): Baseline joining two camera centers lying on the epipolar plane</h5>
  <hr>
  

  <p class="tutorial">Now, let us say that only \( \mathbf{x} \) is known, not \( \mathbf{x'} \). We know that the point \( \mathbf{x'} \) lies in the plane \( \pi \) which is governed by the camera baseline \( \mathbf{CC'} \) and \( \mathbf{\overrightarrow{Cx}} \). Hence the point \( \mathbf{x'} \) lies on the line of intersection \( \mathbf{l'} \) of \( \pi \) with the second image plane. The line \( \mathbf{l'} \) is the image in the second view of the ray back-projected from \( \mathbf{x} \). This line \( \mathbf{l'} \) is called the <em>epipolar line</em> corresponding to \( \mathbf{x} \). The benefit is that you don't need to search for the point corresponding to \( \mathbf{x} \) in the entire image plane as it can be restricted to \( \mathbf{l'} \).</p>


  <ul style="list-style: square;">
    <li class="tutorial"><strong>Epipole</strong> is the point of intersection of the line joining the camera centers with the image plane. (see \( \mathbf{e} \) and \( \mathbf{e'} \) in Fig. 2(a))</li>
    <li class="tutorial"><strong>Epipolar plane</strong> is the plane containing the baseline.</li>
    <li class="tutorial"><strong>Epipolar line</strong> is the intersection of an epipolar plane with the image plane. <em>All the epipolar lines intersect at the epipole.</em></li>
</ul>

<hr>
<a name='estfundmatrix'></a>
<h3 class="tutorial">3.2.2. The Fundamental Matrix \( \mathbf{F} \)</h3><hr>


<p class="tutorial">The \( \mathbf{F} \) matrix is only an algebraic representation of epipolar geometry and can be derived both geometrically <em>(constructing the epipolar line)</em> and arithmetically. (<a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20(Second%20Edition).pdf">See derivation (Page 242)</a>) (<a href="https://www.youtube.com/watch?v=DgGV3l82NTk">Fundamental Matrix Song</a>)</p>

<p class="tutorial">As a result, we obtain:
\[
\mathbf{x}_i'^{\ \mathbf{T}}\mathbf{F} \mathbf{x}_i = 0
\]
where \( i = 1, 2, \ldots, m \). This is known as the epipolar constraint or correspondence condition (also known as the <em>Longuet-Higgins equation</em>). Since \( \mathbf{F} \) is a \( 3 \times 3 \) matrix, we can set up a homogeneous linear system with 9 unknowns:
</p>

\[
\begin{bmatrix} x'_i & y'_i & 1 \end{bmatrix}
\begin{bmatrix} f_{11} & f_{12} & f_{13} \\ f_{21} & f_{22} & f_{23} \\ f_{31} & f_{32} & f_{33} \end{bmatrix}
\begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix} = 0
\]

<p class="tutorial">
\[
x_i x'_i f_{11} + x_i y'_i f_{21} + x_i f_{31} + y_i x'_i f_{12} + y_i y'_i f_{22} + y_i f_{32} + x'_i f_{13} + y'_i f_{23} + f_{33} = 0
\]
</p>

<p class="tutorial">Simplifying for \( m \) correspondences,</p>

\[
\begin{bmatrix} x_1 x'_1 & x_1 y'_1 & x_1 & y_1 x'_1 & y_1 y'_1 & y_1 & x'_1 & y'_1 & 1 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\ x_m x'_m & x_m y'_m & x_m & y_m x'_m & y_m y'_m & y_m & x'_m & y'_m & 1 \end{bmatrix} \begin{bmatrix} f_{11} \\ f_{21} \\ f_{31} \\ f_{12} \\ f_{22} \\ f_{32} \\ f_{13} \\ f_{23} \\ f_{33} \end{bmatrix} = 0
\]

<p class="tutorial"><strong><em>How many points do we need to solve the above equation? Think! Twice!</em></strong> Remember <em>homography</em>, where each point correspondence contributes two constraints? Unlike homography, in \( \mathbf{F} \) matrix estimation, each point only contributes one constraint as the epipolar constraint is a scalar equation. Thus, we require at least 8 points to solve the above homogeneous system. That is why it is known as the <a href="https://en.wikipedia.org/wiki/Eight-point_algorithm">Eight-point algorithm</a>.</p>

<p class="tutorial">With \( N \geq 8 \) correspondences between two images, the fundamental matrix, \( F \) can be obtained as:</p>
<p class="tutorial">By stacking the above equation in a matrix \( A \), the equation \( Ax = 0 \) is obtained. This system of equations can be solved by minimizing the linear least squares using Singular Value Decomposition (SVD), as explained in the <a href="https://cmsc426.github.io/math-tutorial/#svd">Math module</a>. When applying SVD to matrix \( \mathbf{A} \), the decomposition \( \mathbf{USV}^T \) would be obtained, where \( \mathbf{U} \) and \( \mathbf{V} \) are orthonormal matrices, and \( \mathbf{S} \) is a diagonal matrix containing the singular values. The singular values \( \sigma_i \) where \( i \in [1,9], i \in \mathbb{Z} \), are positive and are in decreasing order with \( \sigma_9 = 0 \) since we have 8 equations for 9 unknowns. Thus, the last column of \( \mathbf{V} \) is the true solution, given that \( \sigma_i \neq 0 \ \forall i \in [1,8], i \in \mathbb{Z} \).</p>

<p class="tutorial">However, due to noise in the correspondences, the estimated \( \mathbf{F} \) matrix can be of rank 3, <em>i.e.</em> \( \sigma_9 \neq 0 \). To enforce the rank 2 constraint, the last singular value of the estimated \( \mathbf{F} \) must be set to zero. If \( F \) has full rank, then it will have an empty null-space, <em>i.e.</em>, it won't have any point that is on the entire set of lines. Thus, there wouldn't be any epipoles. See Fig. 3 for full rank comparisons for \( F \) matrices.</p>

<img src="https://cmsc733.github.io/assets/2019/p3/FMatrixRank.png" style="border-radius: 30px;" width="60%">
<h5>Figure 3: F Matrix: Rank 3 vs Rank 2 comparison</h5>
<hr>


<p class="tutorial">In Python, you can use <code>np.linalg.svd</code> to solve \( \mathbf{x} \) from \( \mathbf{Ax} = 0 \)</p>
<style>
  /* Style for the preformatted text block */
  pre {
      font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
      font-size: 16px;
      font-weight: 400;  /* Set to normal weight */
      color: #333;       /* Dark gray color for better readability */
      background-color: #f7f7f7;
      padding: 15px;
      border-radius: 5px;
      border: 1px solid #ddd;
      overflow-x: auto;  /* Horizontal scroll if content overflows */
      text-align: left;
  }

  /* Style for comments */
  .comment {
      color: #6a737d;   /* Lighter color for comments */
      font-style: italic;
      font-size: 16px;
  }
</style>
<pre>
  U, S, Vt = np.linalg.svd(A)
  x = Vt[-1, :]    # <i>Last row of V transpose gives the solution vector</i>
  F = x.reshape((3, 3)).T # Reshape and transpose to construct F matrix
  </pre>

<p class="tutorial"><strong>To summarize, write a function <code>EstimateFundamentalMatrix.py</code> that linearly estimates a fundamental matrix \( F \), such that \( x_2^T F x_1 = 0 \). The fundamental matrix can be estimated by solving the linear least squares \( Ax = 0 \).</strong></p>




<hr>
<a name='estfundmatrix'></a>
<h3 class="tutorial">3.2.3.  Match Outlier Rejection via RANSAC</h3><hr>


<p class="tutorial">Since the point correspondences are computed using SIFT or some other feature descriptors, the data is bound to be noisy and (in general) contains several outliers. Thus, to remove these outliers, we use the RANSAC algorithm <em>(Yes! The same as used in Panorama stitching!)</em> to obtain a better estimate of the fundamental matrix. So, out of all possibilities, the \( \mathbf{F} \) matrix with the maximum number of inliers is chosen.</p>

<p class="tutorial">Below is the pseudo-code that returns the \( \mathbf{F} \) matrix for a set of matching corresponding points (computed using SIFT) which maximizes the number of inliers.</p>

<img src="https://cmsc733.github.io/assets/2019/p3/ransac.png" style="border-radius: 30px;" width="60%">
<h5>Algorithm 1: Get Inliers RANSAC</h5>
<hr>

<img src="https://cmsc733.github.io/assets/2019/p3/featmatchransac.png" style="border-radius: 30px;" width="60%">
<h5>Figure 4: Feature matching after RANSAC. (Green: Selected correspondences; Red: Rejected correspondences)</h5>
<hr>


<p class="tutorial"><strong>Given, \( N \geq 8 \) correspondences between two images, \( x_1 \leftrightarrow x_2 \), implement a function <code>GetInlierRANSAC.py</code> that estimates inlier correspondences using fundamental matrix-based RANSAC.</strong></p>


<hr>
<a name='estE'></a>
<h3 class="tutorial">3.3. Estimate Essential Matrix from Fundamental Matrix</h3><hr>


<p class="tutorial">Since we have computed the \( \mathbf{F} \) using epipolar constraints, we can find the relative camera poses between the two images. This can be computed using the <em>Essential Matrix</em>, \( \mathbf{E} \). The essential matrix is another \( 3 \times 3 \) matrix, but with some additional properties that relate the corresponding points, assuming that the cameras obey the pinhole model (unlike \( \mathbf{F} \)). More specifically,</p>

\[
\mathbf{E} = \mathbf{K}^\mathbf{T} \mathbf{F} \mathbf{K}
\]

<p class="tutorial">where \( \mathbf{K} \) is the camera calibration matrix or camera intrinsic matrix. Clearly, the essential matrix can be extracted from \( \mathbf{F} \) and \( \mathbf{K} \). As in the case of \( \mathbf{F} \) matrix computation, the singular values of \( \mathbf{E} \) are not necessarily \( (1, 1, 0) \) due to noise in \( \mathbf{K} \). This can be corrected by reconstructing it with \( (1, 1, 0) \) singular values, <em>i.e.</em></p>

\[
\mathbf{E} = U \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} V^\mathbf{T}
\]

<p class="tutorial"><em>It is important to note that \( \mathbf{F} \) is defined in the original image space (i.e., pixel coordinates), whereas \( \mathbf{E} \) is in the normalized image coordinates. Normalized image coordinates have the origin at the optical center of the image. Also, relative camera poses between two views can be computed using the \( \mathbf{E} \) matrix. Moreover, \( \mathbf{F} \) has 7 degrees of freedom while \( \mathbf{E} \) has 5 as it takes camera parameters into account. (<a href="http://users.cecs.anu.edu.au/~hongdong/new5pt_cameraREady_ver_1.pdf">5-Point Motion Estimation Made Easy</a>)</em></p>

<p class="tutorial"><strong>Given \( F \), estimate the essential matrix \( E = K^\mathbf{T} F K \) by implementing the function <code>EssentialMatrixFromFundamentalMatrix.py</code>.</strong></p>



<hr>
<a name='essential'></a>
<h3 class="tutorial">3.4. Estimate Camera Pose from Essential Matrix</h3><hr>

<p class="tutorial">The camera pose consists of 6 degrees-of-freedom (DOF): Rotation (Roll, Pitch, Yaw) and Translation (X, Y, Z) of the camera with respect to the world. Since the \( \mathbf{E} \) matrix is identified, the four camera pose configurations: \( (C_1, R_1) \), \( (C_2, R_2) \), \( (C_3, R_3) \), and \( (C_4, R_4) \), where \( C \in \mathbb{R}^3 \) is the camera center and \( R \in SO(3) \) is the rotation matrix, can be computed. Thus, the camera pose can be written as:</p>

\[
P = K R \begin{bmatrix} I_{3 \times 3} & -C \end{bmatrix}
\]

<p class="tutorial">These four pose configurations can be computed from the \( \mathbf{E} \) matrix. Let \( \mathbf{E} = UDV^T \) and \( W = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} \). The four configurations can be written as:</p>

<ol class="tutorial">
    <li>\( C_1 = U(:, 3) \) and \( R_1 = UWV^T \)</li>
    <li>\( C_2 = -U(:, 3) \) and \( R_2 = UWV^T \)</li>
    <li>\( C_3 = U(:, 3) \) and \( R_3 = UW^TV^T \)</li>
    <li>\( C_4 = -U(:, 3) \) and \( R_4 = UW^TV^T \)</li>
</ol>

<p class="tutorial"><strong>It is important to note that \( \det(R) = 1 \). If \( \det(R) = -1 \), the camera pose must be corrected, <em>i.e.</em>, \( C = -C \) and \( R = -R \). Implement the function <code>ExtractCameraPose.py</code>, given \( E \).</strong></p>


<hr>
<a name='tri'></a>
<h3 class="tutorial">3.5. Triangulation Check for Cheirality Condition</h3><hr>

<p class="tutorial">In the previous section, we computed four different possible camera poses for a pair of images using the essential matrix. In this section, we will triangulate the 3D points, given two camera poses.</p>

<p class="tutorial"><strong>Given two camera poses, \( (C_1, R_1) \) and \( (C_2, R_2) \), and correspondences, \( x_1 \leftrightarrow x_2 \), triangulate 3D points using linear least squares. Implement the function <code>LinearTriangulation.py</code>.</strong></p>

<p class="tutorial">However, to find the <em>correct</em> unique camera pose, we need to resolve the ambiguity. This can be accomplished by checking the <strong>cheirality condition</strong>, <em>i.e.</em>, <em>the reconstructed points must be in front of the cameras</em>.</p>

<p class="tutorial">To check the cheirality condition, triangulate the 3D points (given two camera poses) using <strong>linear least squares</strong> to verify the sign of the depth \( Z \) in the camera coordinate system with respect to the camera center. A 3D point \( X \) is in front of the camera if and only if:</p>

\[
r_3 \cdot (\mathbf{X} - \mathbf{C}) > 0
\]

<p class="tutorial">where \( r_3 \) is the third row of the rotation matrix (z-axis of the camera). Not all triangulated points satisfy this condition due to the presence of correspondence noise. The best camera configuration, \( (C, R, X) \), is the one that produces the maximum number of points satisfying the cheirality condition.</p>

<img src="https://cmsc733.github.io/assets/2019/p3/lintria.png" style="border-radius: 30px;" width="50%">
<h5>Figure 5: Initial triangulation plot with ambiguity, showing all four possible camera poses.</h5>
<hr>


<p class="tutorial"><strong>Given four camera pose configurations and their triangulated points, find the unique camera pose by checking the cheirality condition – the reconstructed points must be in front of the cameras (implement the function <code>DisambiguateCameraPose.py</code>).</strong></p>






<hr>
<a name='nonlintri'></a>
<h3 class="tutorial">3.5.1. Non-Linear Triangulation</h3><hr>



<p class="tutorial">Given two camera poses and linearly triangulated points, \( X \), the locations of the 3D points that minimize the reprojection error (Recall <a href="https://cmsc426.github.io/pano/#reproj">Project 2</a>) can be refined. The linear triangulation minimizes the algebraic error. However, the reprojection error is a geometrically meaningful error and can be computed by measuring the error between the measurement and the projected 3D point:</p>

\[
\underset{x}{\operatorname{min}} \sum_{j=1,2} \left( u^j - \frac{P_1^{jT} \widetilde{X}}{P_3^{jT} X} \right)^2 + \left( v^j - \frac{P_2^{jT} \widetilde{X}}{P_3^{jT} X} \right)^2
\]

<p class="tutorial">Here, \( j \) is the index of each camera, \( \widetilde{X} \) is the homogeneous representation of \( X \). \( P_i^T \) is each row of the camera projection matrix, \( P \). This minimization is highly nonlinear due to the divisions. The initial guess of the solution, \( X_0 \), is estimated via linear triangulation to minimize the cost function. This minimization can be solved using nonlinear optimization functions such as <code>scipy.optimize.leastsq</code> or <code>scipy.optimize.least_squares</code> in the SciPy library.</p>

<img src="https://cmsc733.github.io/assets/2019/p3/nonlintria.png" style="border-radius: 30px;" width="50%">
<h5>Figure 6: Comparison between non-linear vs linear triangulation.</h5>
<hr>

<p class="tutorial"><strong>Given two camera poses and linearly triangulated points, \( X \), refine the locations of the 3D points that minimize the reprojection error (implement the function <code>NonlinearTriangulation.py</code>).</strong></p>


<hr>
<a name='pnp'></a>
<h3 class="tutorial">3.6. Perspective-\(n\)-Points</h3><hr>

<p class="tutorial">Now, since we have a set of \( n \) 3D points in the world, their 2D projections in the image, and the intrinsic parameter, the 6 DOF camera pose can be estimated using linear least squares. This fundamental problem is generally known as <em>Perspective</em>-\( n \)-<em>Point</em> (PnP). For there to exist a solution, \( n \geq 3.\) There are multiple methods to solve the P\( n \)P problem, and most of them assume that the camera is calibrated. Methods such as <a href="https://pdfs.semanticscholar.org/f1d6/2775d4a51161663ff9453b37bb21a1263f25.pdf">Unified P\( n \)P</a> (or UPnP) do not adhere to this assumption, as they estimate both intrinsic and extrinsic parameters. In this section, you will explore a simpler version of PnP. You will register a new image given 2D-3D correspondences, i.e., \( X \leftrightarrow x \), followed by nonlinear optimization.</p>



<hr>
<a name='pnp'></a>
<h3 class="tutorial">3.6.1 Linear Camera Pose Estimation</h3><hr>


<p class="tutorial">Given 2D-3D correspondences, \( X \leftrightarrow x \), and the intrinsic parameter \( K \), estimate the camera pose using linear least squares (implement the function <code>LinearPnP.py</code>). 2D points can be normalized by the intrinsic parameter to isolate camera parameters, \( (C, R) \), i.e., \( K^{-1} x \). A linear least squares system that relates the 3D and 2D points can be solved for \( (t, R) \) where \( t = -R^T C \). Since the linear least square solution does not enforce orthogonality of the rotation matrix, \( R \in SO(3) \), the rotation matrix must be corrected by \( R = UV^T \) where \( R = UDV^T \). If the corrected rotation has a determinant of \( -1 \), then \( R = -R \). This linear PnP requires at least 6 correspondences. <em>(Think why?)</em></p>


<img src="https://cmsc733.github.io/assets/2019/p3/nonlintria.png" style="border-radius: 30px;" width="50%">
<h5>Figure 7: Plot of the camera poses with feature points. Different colors represent feature correspondences from different pairs of images. Blue points are features from Image 1 and Image 2; Red points are features from Image 2 and Image 3, etc.
</h5>
<hr>


<hr>
<a name='pnp'></a>
<h3 class="tutorial">3.6.2 PnP RANSAC</h3><hr>
<p class="tutorial">PnP is prone to error as there are outliers in the given set of point correspondences. To overcome this error, we can use RANSAC (yes, again!) to make our camera pose estimation more robust to outliers. To formalize, given \( N \geq 6 \) 3D-2D correspondences, \( X \leftrightarrow x \), implement the following function that estimates the camera pose \( (C, R) \) via RANSAC (implement the function <code>PnPRANSAC.py</code>).</p>

<p class="tutorial">The algorithm below depicts the solution using RANSAC.</p>

<img src="https://cmsc733.github.io/assets/2019/p3/pnpransac.png" style="border-radius: 30px;" width="50%">
<h5>Algorithm 2: PnP RANSAC</h5>
<hr>

<p class="tutorial">Just like in triangulation, since we have the linearly estimated camera pose, we can refine the camera pose to minimize the reprojection error (Linear PnP only minimizes the algebraic error).</p>


<hr>
<a name='pnp'></a>
<h3 class="tutorial">3.6.3 Nonlinear PnP</h3><hr>

<p class="tutorial">Given \( N \geq 6 \) 3D-2D correspondences, \( X \leftrightarrow x \), and a linearly estimated camera pose, \( (C, R) \), refine the camera pose to minimize the reprojection error (implement the function <code>NonlinearPnP.py</code>). The linear PnP minimizes algebraic error. The reprojection error, a geometrically meaningful error, is computed by measuring the error between the measurement and the projected 3D point:</p>

\[
\underset{C, R}{\operatorname{min}} \sum_{j=1}^{J} \left( u^j - \frac{P_1^{jT} \widetilde{X_j}}{P_3^{jT} \widetilde{X_j}} \right)^2 + \left( v^j - \frac{P_2^{jT} \widetilde{X_j}}{P_3^{jT} \widetilde{X_j}} \right)^2
\]

<p class="tutorial">Here, \( \widetilde{X} \) is the homogeneous representation of \( X \). \( P_i^T \) represents each row of the camera projection matrix, \( P \), which is computed as \( P = K R [I_{3 \times 3} - C] \). A compact representation of the rotation matrix using a quaternion is a better choice to enforce orthogonality of the rotation matrix, \( R = R(q) \), where \( q \) is a four-dimensional quaternion. Thus, we have:</p>

\[
\underset{C, q}{\operatorname{min}} \sum_{j=1}^{J} \left( u^j - \frac{P_1^{jT} \widetilde{X_j}}{P_3^{jT} \widetilde{X_j}} \right)^2 + \left( v^j - \frac{P_2^{jT} \widetilde{X_j}}{P_3^{jT} \widetilde{X_j}} \right)^2
\]

<p class="tutorial">This minimization is highly nonlinear due to the divisions and quaternion parameterization. The initial guess of the solution, \( (C_0, R_0) \), estimated via linear PnP, is needed to minimize the cost function. This minimization can be solved using a nonlinear optimization function such as <code>scipy.optimize.leastsq</code> or <code>scipy.optimize.least_squares</code> in the SciPy library.</p>



<hr>
<a name='ba'></a>
<h3 class="tutorial">3.7. Bundle Adjustment</h3><hr>
<p class="tutorial">Once you have computed all the camera poses and 3D points, we need to refine the poses and 3D points together, initialized by the previous reconstruction, by minimizing the reprojection error.</p>


<img src="https://cmsc733.github.io/assets/2019/p3/BA.png" style="border-radius: 30px;" width="50%">
<h5>Figure 7: The final reconstructed scene after Sparse Bundle Adjustment (SBA).</h5>
<hr>





<hr>
<a name='ba'></a>
<h3 class="tutorial">3.7.1 Visibility Matrix</h3><hr>
<p class="tutorial">Find the relationship between a camera and point, and construct an \( I \times J \) binary matrix, \( V \), where \( V_{ij} \) is one if the \( j^{th} \) point is visible from the \( i^{th} \) camera and zero otherwise (implement the function <code>BuildVisibilityMatrix.py</code>).</p>

<h3 class="tutorial">3.7.2 Bundle Adjustment Optimization</h3><hr>
<p class="tutorial">Given initialized camera poses and 3D points, refine them by minimizing the reprojection error (implement the function <code>BundleAdjustment.py</code>). Bundle adjustment refines camera poses and 3D points simultaneously by minimizing the following reprojection error over \( C_{i_{i=1}}^I \), \( q_{i_{i=1}}^I \), and \( X_{j_{j=1}}^J \).</p>

<p class="tutorial">The optimization problem can be formulated as follows:</p>

\[
\underset{\{C_i, q_i\}_{i=1}^I, \{X\}_{j=1}^J}{\operatorname{min}} \sum_{i=1}^I \sum_{j=1}^J V_{ij} \left( \left( u^j - \frac{P_1^{jT} \tilde{X}}{P_3^{jT} \tilde{X}} \right)^2 + \left( v^j - \frac{P_2^{jT} \tilde{X}}{P_3^{jT} \tilde{X}} \right)^2 \right)
\]

<p class="tutorial">where \( V_{ij} \) is the visibility matrix.</p>

<p class="tutorial">Clearly, solving such a method to compute the structure from motion is complex and slow <em>(can take from several minutes for only 8-10 images)</em>. This minimization can be solved using nonlinear optimization functions such as <code>scipy.optimize.leastsq</code>, but will be extremely slow due to the number of parameters. Sparse Bundle Adjustment toolboxes, such as <a href="https://buildmedia.readthedocs.org/media/pdf/python-sba/latest/python-sba.pdf">pySBA</a> and <a href="https://scipy-cookbook.readthedocs.io/items/bundle_adjustment.html">large-scale BA in scipy</a>, are designed to solve such optimization by exploiting the sparsity of the visibility matrix, \( V \). Note that a small number of entries in \( V \) are one because a 3D point is visible from a small subset of images. Using the sparse bundle adjustment package is not trivial but would be much faster than one you write. For SBA, you are allowed to use any optimization library.</p>


<hr>
<h2 class="tutorial">4. Putting the pipeline together</h2><hr>
<p class="tutorial">Write a program <code>Wrapper.py</code> that runs the full pipeline of structure from motion based on the above algorithm.</p>

<p class="tutorial">Also, compare your result against COLMAP or VSfM output.</p>

<hr><h4 class="tutorial">Project Overview</h4><hr>
<img src="https://cmsc733.github.io/assets/2019/p3/summary.png" style="border-radius: 30px;" width="50%">
<h5>Figure 8: The overview.</h5>

<a name="testset"></a>
<hr><h2 class="tutorial">5. Notes about the Sample Data</h2><hr>
<p class="tutorial">Run your SfM algorithm on the images provided <a href="https://drive.google.com/file/d/1peELDAqJLlWbF6cxd6yW2QIUKgh6I3Yw/view?usp=sharing">here</a>. The data given to you consists of a set of 6 images of a building in front of Levine Hall at UPenn, captured using a GoPro Hero 3 with fisheye lens distortion corrected. Keypoint matching (SIFT keypoints and descriptors) data is also provided in the same folder for pairs of images. The data folder contains 5 matching files named <code>matching*.txt</code>, where <code>*</code> refers to numbers from 1 to 5. For example, <code>matching3.txt</code> contains the matching between the third image and the fourth, fifth, and sixth images, i.e., \( \mathcal{I}_3 \leftrightarrow \mathcal{I}_4 \), \( \mathcal{I}_3 \leftrightarrow \mathcal{I}_5 \), and \( \mathcal{I}_3 \leftrightarrow \mathcal{I}_6 \). Therefore, <code>matching6.txt</code> does not exist because it would represent a match of the image with itself.</p>

<p class="tutorial">The format of the matching files is described below. Each matching file is formatted as follows for the \( i^{th} \) matching file:</p>

<ul class="tutorial">
  <li><strong>nFeatures:</strong> the number of feature points of the \( i^{th} \) image - each following row specifies matches across images for a feature location in the \( i^{th} \) image.</li>
  <li><strong>Each Row:</strong> (the number of matches for the \( j^{th} \) feature) (Red Value) (Green Value) (Blue Value) \( u_{\texttt{current image}} \) \( v_{\texttt{current image}} \) (image id) \( u_{\texttt{image id image}} \) \( v_{\texttt{image id image}} \) ...</li>
</ul>

<p class="tutorial">An example of <code>matching1.txt</code> is given below:</p>
<pre>
nFeatures: 2002
3 137 128 105 454.740000 392.370000 2 308.570000 500.320000 4 447.580000 479.360000
2 137 128 105 454.740000 392.370000 4 447.580000 479.360000
</pre>

<p class="tutorial">The images are taken at 1280 × 960 resolution, and the camera intrinsic parameters \( K \) are provided in the <code>calibration.txt</code> file. You will implement this full pipeline guided by the functions described in the following sections.</p>

<!-- <p class="tutorial"><strong>For extra credit:</strong> Capture a set of images and run your SfM algorithm. <strong>DO NOT</strong> use images from the internet. Analyze the success and failure of your algorithm and showcase this in your report. Note: You need to capture images, calibrate them, and undistort them. You may use any in-built calibration tool, such as MATLAB's calibration tool in the Computer Vision toolbox.</p> -->

<a name="sub"></a>
<hr><h2 class="tutorial">6. Submission Guidelines</h2><hr>

<h3 class="tutorial">Download the Starter Code and Data from <a href="https://drive.google.com/file/d/1peELDAqJLlWbF6cxd6yW2QIUKgh6I3Yw/view?usp=sharing">here</a>.</h3>

<a name="files"></a>
<h3 class="tutorial">6.1. File Tree and Naming</h3>
<p class="tutorial">Your submission on ELMS/Canvas must be a <code>zip</code> file, following the naming convention <code>YourDirectoryID_p3.zip</code>. If your email ID is <code>abc@colorado.edu</code>, then your <code>DirectoryID</code> is <code>abc</code>. For our example, the submission file should be named <code>abc_p1.zip</code>. The file <strong>must have the following directory structure</strong> because we will be autograding assignments. The file to run for your project should be called <code>Wrapper.py</code>. You may include helper functions in subfolders as needed; ensure you use relative paths and include default values for command line arguments in <code>Wrapper.py</code>. Please provide detailed instructions for running your code in the <code>README.md</code> file.

<pre>
YourDirectoryID_p3.zip
│   README.md
|   Code/
|   ├── GetInliersRANSAC.py
|   ├── EstimateFundamentalMatrix.py
|   ├── EssentialMatrixFromFundamentalMatrix.py
|   ├── ExtractCameraPose.py
|   ├── LinearTriangulation.py
|   ├── DisambiguateCameraPose.py
|   ├── NonlinearTriangulation.py
|   ├── PnPRANSAC.py
|   ├── NonlinearPnP.py
|   ├── BuildVisibilityMatrix.py
|   ├── BundleAdjustment.py
|   ├── Wrapper.py
|   ├── Any subfolders you want along with files
|   ├── Wrapper.py 
|   Data/
|   ├── BundleAdjustmentOutputForAllImage/
|   ├── FeatureCorrespondenceOutputForAllImageSet/
|   ├── LinearTriangulationOutputForAllImageSet/
|   ├── NonLinearTriangulationOutputForAllImageSet/
|   ├── PnPOutputForAllImageSetShowingCameraPoses/
|   ├── Imgs/
└── Report.pdf
</pre>

<a name="report"></a>
<hr><h3 class="tutorial">6.2. Report</h3><hr>

<p class="tutorial">The final dataset for this project will release one week after the release. For each section of the project, explain briefly what you did, and describe any interesting problems you encountered and/or solutions you implemented. You must include the following details in your writeup:</p>

<ol class="tutorial" style="line-height: normal;">
  <li>Make your report extremely detailed with reprojection error after each step (Linear, Non-linear triangulation, Linear, Non-linear PnP before and after BA, etc.). Describe all the steps (anything that is not obvious) and any other observations in your report.</li>
  <li>Your report <strong>MUST</strong> be typeset in LaTeX in the IEEE Tran format provided in the <code>Draft</code> folder and should be of conference-quality paper.</li>
  <li>Present the data you collected in <code>Data/Imgs/</code>.</li>
  <li>Present failure cases and explanations, if any.</li>
  <li>Do not use any function that directly implements a part of the pipeline. If you have any doubts, please contact us via Piazza.</li>
</ul>



<hr>
<p class="tutorial">This project is derived from UMD's CMSC733 and Upenn's CIS580 course.</p> 






<div class="row text-center"><hr><h2 class="section-title">Collaboration Policy and Honor Code</h2><hr><div class="pub-img"></div></div>  
<div class="pub-text"><h4 style="text-align:justify;">
Collaboration is encouraged, but one should know the difference between collaboration and cheating. Cheating is prohibited and will carry serious consequences. Cheating may be defined as using or attempting to use unauthorized assistance, material, or study aids in academic work or examinations. Some examples of cheating are: collaborating on an take-home exam or homework unless explicitly allowed; copying homework; handing in someone else's work as your own; and plagiarism. You are welcome to collaborate with your peers on Piazza and in person. However it's important that the work you submit is an expression of your understanding, and not merely something you copied from a peer. So, we place strict limits on collaboration: Firstly, you must clearly cite your collaborators by name at the top of your report. This includes Piazza posts reference. You may not share or copy each other's code. You can discuss how your code works, and the concepts it implements, but you can't just show someone your code. You may use free and publicly available sources, such as books, journal and conference publications, and web pages, as research material for your answers. (You will not lose points for using external sources.) You may not use any service that involves payment, and you must clearly and explicitly cite all outside sources and materials that you made use of. We consider the use of uncited external sources as portraying someone else's work as your own, and as such it is a violation of the University's policies on academic dishonesty. Instances will be dealt with harshly and typically result in a failing course grade. Unless otherwise specified, you should assume that that the <a href="https://catalog.colorado.edu/graduate/academic-integrity/" style="color:#b39e07"><b>CU Boulder Code of Academic Integrity</b></a> applies. Unless otherwise specified, you should assume that that the CU Boulder Code of Academic Integrity applies.

</div>
</div>
</section>

<hr><br>

<section class="container"><div class="pub-authors">
  <h3>For previous courses offered by Chahat Deep Singh, please visit <a href="https://chahatdeep.github.io/teaching.html#teaching"><b style="color: #b39e07;">here</b></a>.</h3>
</div>
</section>
  <hr>



  <footer-component></footer-component>

  <!-- <div id="preloader">
    <div id="status">
      <div class="status-mes"><center><h3>Loading...</h3><br><div class="loader"></center></div></div>
    </div>
  </div> -->


    <!-- jQuery -->
    <script src="../js/jquery.js"></script>
    <!-- Bootstrap Core JavaScript -->
    <script src="../js/bootstrap.min.js"></script>
    <!-- owl.carousel -->
    <script src="../owl.carousel/owl.carousel.min.js"></script>
    <!-- Magnific-popup -->
    <script src="../js/jquery.magnific-popup.min.js"></script>
    <!-- Offcanvas Menu -->
    <script src="../js/hippo-offcanvas.js"></script>
    <!-- inview -->
    <script src="../js/jquery.inview.min.js"></script>
    <!-- stellar -->
    <script src="../js/jquery.stellar.js"></script>
    <!-- countTo -->
    <script src="../js/jquery.countTo.js"></script>
    <!-- classie -->
    <script src="../js/classie.js"></script>
    <!-- selectFx -->
    <script src="../js/selectFx.js"></script>
    <!-- sticky kit -->
    <script src="../js/jquery.sticky-kit.min.js"></script>
    <!-- GOGLE MAP -->
    <script src="https://maps.googleapis.com/maps/api/js"></script>
    <!--TWITTER FETCHER-->
    <!-- <script src="js/twitterFetcher_min.js"></script> -->
    <!-- Custom Script -->
    <script src="../js/scripts.js"></script>

    
	</body>
</html>

